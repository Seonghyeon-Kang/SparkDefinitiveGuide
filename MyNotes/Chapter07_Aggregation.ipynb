{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 구매 이력 데이터를 사용해 파티션을 휠씬 적은 수로 분할할 수 있도록 리파티셔닝\n",
    "+ 빠르게 접근할 수 있도록 캐싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 구매 이력 데이터 \"\"\"\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"./data/retail-data/all/*.csv\")\\\n",
    "    .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 집계 함수\n",
    "+ org.apache.spark.sql.functions 패키지 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "df.select(count(\"StockCode\")).show() # null 로우는 제외\n",
    "df.select(count(\"*\")).show()         # null 로우를 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 countDistinct\n",
    "+ 고유레코드 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 approx_count_distinct\n",
    "+ 근사치로 구하지만 연산 속도가 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            4079|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show() # 0.1은 최대 추정 오류율\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.01)).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4 first와 last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show(1) # null도 감안하려면 True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.5 min과 max\n",
    "+ 문자열도 동작이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n",
      "+--------------------+-----------------+\n",
      "|    min(Description)| max(Description)|\n",
      "+--------------------+-----------------+\n",
      "| 4 PURPLE FLOCK D...|wrongly sold sets|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show(1)\n",
    "df.select(min(\"Description\"), max(\"Description\")).show(1) # 문자열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.6 sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.7 sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show(1) # 고유값을 합산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.8 avg\n",
    "+ avg, mean 함수로 평균을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+-----------------+\n",
      "|(total_purchases / total_transcations)|   avg_purchases|mean_transcations|\n",
      "+--------------------------------------+----------------+-----------------+\n",
      "|                      9.55224954743324|9.55224954743324| 9.55224954743324|\n",
      "+--------------------------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transcations\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_transcations\"),    \n",
    ").selectExpr(\n",
    "    \"total_purchases / total_transcations\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_transcations\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.9 분산과 표준편차\n",
    "+ 표본표준분산 및 편차: variance, stddev\n",
    "+ 모표준분산 및 편차 : var_pop, stddev_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)| var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "|47559.391409298754|   218.08115785023418|47559.391409298754|   218.08115785023418|47559.303646609056|  218.08095663447796|\n",
      "+------------------+---------------------+------------------+---------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance, stddev\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "\n",
    "df.select(variance(\"Quantity\"), stddev(\"Quantity\"),      \n",
    "          var_samp(\"Quantity\"), stddev_samp(\"Quantity\"), # 위와 동일\n",
    "          var_pop(\"Quantity\"), stddev_pop(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "|var_samp(Quantity)|stddev_samp(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)|var_pop(Quantity)|stddev_pop(Quantity)|\n",
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "|               NaN|                  NaN|               NaN|                  NaN|              0.0|                 0.0|\n",
      "+------------------+---------------------+------------------+---------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df.select(\"*\").take(1)).select(variance(\"Quantity\"), stddev(\"Quantity\"),      \n",
    "          var_samp(\"Quantity\"), stddev_samp(\"Quantity\"), # 위와 동일\n",
    "          var_pop(\"Quantity\"), stddev_pop(\"Quantity\")).show() # 1일 때는 NaN이 나옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.10 비대칭도와 첨도\n",
    "+ 비대칭도와 첨도 : https://www.youtube.com/watch?time_continue=2&v=g9VOhfy2WWY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.11 공분산과 상관관계\n",
    "+ 표본공분산(cover_samp), 모공분산(cover_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "|     4.912186085635685E-4|            1052.7260778741693|             1052.7280543902734|\n",
      "+-------------------------+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.12 복합 데이터 타입의 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|collect_list(Country)|collect_set(Country)|\n",
      "+---------------------+--------------------+\n",
      "| [United Kingdom, ...|[Portugal, Italy,...|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set, size\n",
    "\n",
    "df.select(collect_list(\"Country\"), collect_set(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------------+\n",
      "|size(collect_list(Country))|size(collect_set(Country))|\n",
      "+---------------------------+--------------------------+\n",
      "|                     541909|                        38|\n",
      "+---------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(size(collect_list(\"Country\")), size(collect_set(\"Country\"))).show() # 각 컬럼의 복합데이터 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Country)|\n",
      "+-----------------------+\n",
      "|                     38|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"Country\")).show() # 중복없이 카운트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 그룹화\n",
    "+ 하나 이상의 컬럼을 그룹화하여 RelationalGroupedDataset 반환\n",
    "+ 집계 연산을 수행하는 두 번째 단계에서는 DataFrame이 반환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   536596|      null|    6|\n",
      "|   537252|      null|    1|\n",
      "|   538041|      null|    1|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 표현식을 이용한 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----+---------------+\n",
      "|InvoiceNo|CustomerId|guan|count(Quantity)|\n",
      "+---------+----------+----+---------------+\n",
      "|   536846|     14573|  76|             76|\n",
      "|   537026|     12395|  12|             12|\n",
      "|   537883|     14437|   5|              5|\n",
      "|   538068|     17978|  12|             12|\n",
      "|   538279|     14952|   7|              7|\n",
      "|   538800|     16458|  10|             10|\n",
      "|   538942|     17346|  12|             12|\n",
      "|  C539947|     13854|   1|              1|\n",
      "|   540096|     13253|  16|             16|\n",
      "|   540530|     14755|  27|             27|\n",
      "|   541225|     14099|  19|             19|\n",
      "|   541978|     13551|   4|              4|\n",
      "|   542093|     17677|  16|             16|\n",
      "|   536596|      null|   6|              6|\n",
      "|   537252|      null|   1|              1|\n",
      "|   538041|      null|   1|              1|\n",
      "|   543188|     12567|  63|             63|\n",
      "|   543590|     17377|  19|             19|\n",
      "|  C543757|     13115|   1|              1|\n",
      "|  C544318|     12989|   1|              1|\n",
      "+---------+----------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").agg(\n",
    "    count(\"Quantity\").alias(\"guan\"),\n",
    "    expr(\"count(Quantity)\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 맵을 이용한 그룹화\n",
    "+ 파이선의 딕셔너리 데이터 타입을 활용하여 집계함수의 표현이 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|InvoiceNo|stddev_pop(Quantity)|\n",
      "+---------+--------------------+\n",
      "|   536596|  1.1180339887498947|\n",
      "|   536938|  20.698023172885524|\n",
      "|   537252|                 0.0|\n",
      "|   537691|   5.597097462078001|\n",
      "|   538041|                 0.0|\n",
      "|   538184|   8.142590198943392|\n",
      "|   538517|  2.3946659604837897|\n",
      "|   538879|  11.811070444356483|\n",
      "|   539275|  12.806248474865697|\n",
      "|   539630|  10.225241100118645|\n",
      "|   540499|  2.6653642652865788|\n",
      "|   540540|  1.0572457590557278|\n",
      "|  C540850|                 0.0|\n",
      "|   540976|   6.496760677872902|\n",
      "|   541432|  10.825317547305483|\n",
      "|   541518|  20.550782784878713|\n",
      "|   541783|   8.467657556242811|\n",
      "|   542026|   4.853406592853679|\n",
      "|   542375|  3.4641016151377544|\n",
      "|  C542604|  15.173990905493518|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg({\"Quantity\":\"avg\", \"Quantity\":\"stddev_pop\"}).show() # 키 값이 충돌됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 윈도우 함수\n",
    "+ group-by 함수를 사용하면 모든 로우 레코드가 단일 그룹으로만 이동하지만, 윈도우 함수는 하나 이상의 프레임에 할당될 수 있음\n",
    "+ 가장 흔한 예시로는 하루는 나타내는 값의 롤링 평균을 구하는 예시\n",
    "+ https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 예시 DataFrame : 날짜 정보를 가진 date 컬럼 추가 \"\"\"\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n",
    "dfWithDate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec = Window\\\n",
    "    .partitionBy(\"CustomerId\", \"date\")\\\n",
    "    .orderBy(desc(\"Quantity\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow) # 파티션의 처음부터 현재 로우까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxDenseRank|\n",
      "+----------+----------+--------+------------+-----------------+------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|       74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|       74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|          36|\n",
      "|     12347|2010-12-07|      30|           2|                2|          36|\n",
      "|     12347|2010-12-07|      24|           3|                3|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|      12|           4|                4|          36|\n",
      "|     12347|2010-12-07|       6|          17|                5|          36|\n",
      "|     12347|2010-12-07|       6|          17|                5|          36|\n",
      "+----------+----------+--------+------------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "    .select(col(\"CustomerId\"), col(\"date\"), col(\"Quantity\"),\n",
    "            purchaseRank.alias(\"quantityRank\"),           # 중복 순위를 감안\n",
    "            purchaseDenseRank.alias(\"quantityDenseRank\"), # 중복 순위를 감안하지 않음\n",
    "            maxPurchaseQuantity.alias(\"maxDenseRank\")\n",
    "           ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 그룹화 셋\n",
    "+ 여러 그룹에 걸쳐 집계를 결합하는 저수준 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+\n",
      "|stockCode|CustomerId|sumOfQuantity|\n",
      "+---------+----------+-------------+\n",
      "|    84826|     13256|        12540|\n",
      "|    22197|     17949|        11692|\n",
      "|    84077|     16333|        10080|\n",
      "|    17003|     16422|        10077|\n",
      "|    21915|     16333|         8120|\n",
      "|    16014|     16308|         8000|\n",
      "|    22616|     17306|         6624|\n",
      "|    22189|     18102|         5946|\n",
      "|    84077|     12901|         5712|\n",
      "|    18007|     14609|         5586|\n",
      "|    22197|     12931|         5340|\n",
      "|    22469|     17450|         5286|\n",
      "|    84879|     12931|         5048|\n",
      "|    23084|     14646|         4801|\n",
      "|    22693|     16029|         4800|\n",
      "|    21137|     16210|         4728|\n",
      "|    22616|     17381|         4704|\n",
      "|   85099B|     15769|         4700|\n",
      "|    21787|     12901|         4632|\n",
      "|    22629|     14646|         4492|\n",
      "+---------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 재고 코드(stockCode)와 고객(CustomerId)별 총 수량 얻기 \"\"\"\n",
    "dfNoNull = dfWithDate.na.drop()\n",
    "dfNoNull.createOrReplaceTempView('dfNoNull')\n",
    "\n",
    "# drop 후에도 CustomerId에 null이 포함되는 이유는 ? na를 추가해야 함.\n",
    "dfNoNull\\\n",
    "    .groupBy(col(\"stockCode\"), col(\"CustomerId\")).agg(\n",
    "        sum(\"Quantity\").alias(\"sumOfQuantity\")\n",
    "    ).orderBy(desc(\"sumOfQuantity\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 롤업\n",
    "+ 다양한 컬럼을 그룹화 키로 설정하면 데이터셋에서 볼 수 있는 실제 조합을 살펴볼 수 있음\n",
    "+ 컬럼을 계층적으로 다룸\n",
    "+ null 값을 가진 로우에서 전체 날짜의 합계를 확인할 수 있음\n",
    "+ 모두 null인 로우는 두 컬럼에 속한 레코드의 전체 합계를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|null|   null|       4906888|\n",
      "+----+-------+--------------+\n",
      "\n",
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       4906888|\n",
      "|2010-12-01|   null|         24032|\n",
      "|2010-12-02|   null|         20855|\n",
      "|2010-12-03|   null|         11548|\n",
      "|2010-12-05|   null|         16394|\n",
      "|2010-12-06|   null|         16095|\n",
      "|2010-12-07|   null|         19351|\n",
      "|2010-12-08|   null|         21275|\n",
      "|2010-12-09|   null|         16904|\n",
      "|2010-12-10|   null|         15388|\n",
      "|2010-12-12|   null|         10561|\n",
      "|2010-12-13|   null|         15234|\n",
      "|2010-12-14|   null|         17108|\n",
      "|2010-12-15|   null|         18169|\n",
      "|2010-12-16|   null|         29482|\n",
      "|2010-12-17|   null|         10517|\n",
      "|2010-12-19|   null|          3735|\n",
      "|2010-12-20|   null|         12617|\n",
      "|2010-12-21|   null|         10888|\n",
      "|2010-12-22|   null|          3053|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    "    .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    "    .orderBy(\"Date\")\n",
    "rolledUpDF.where(expr(\"Date IS NULL\")).show()    # null 값을 가진 로우는 전체 날짜의 합계 확인\n",
    "rolledUpDF.where(expr(\"Country IS NULL\")).show() # 모두 null인 로우는 두 컬럼에 속한 레코드의 전체 합계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 큐브\n",
    "+ 계층적이 아닌 모든 차원에 대해 동일한 작업을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+\n",
      "|Date|             Country|total_quantity|\n",
      "+----+--------------------+--------------+\n",
      "|null|  European Community|           497|\n",
      "|null|              Norway|         19247|\n",
      "|null|     Channel Islands|          9479|\n",
      "|null|             Lebanon|           386|\n",
      "|null|           Singapore|          5234|\n",
      "|null|United Arab Emirates|           982|\n",
      "|null|                 USA|          1034|\n",
      "|null|             Denmark|          8188|\n",
      "|null|              Cyprus|          6317|\n",
      "|null|               Spain|         26824|\n",
      "|null|      Czech Republic|           592|\n",
      "|null|             Germany|        117448|\n",
      "|null|                 RSA|           352|\n",
      "|null|           Australia|         83653|\n",
      "|null|            Portugal|         16044|\n",
      "|null|         Unspecified|          1789|\n",
      "|null|               Japan|         25218|\n",
      "|null|             Finland|         10666|\n",
      "|null|               Italy|          7999|\n",
      "|null|              Greece|          1556|\n",
      "+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       4906888|\n",
      "|2010-12-01|   null|         24032|\n",
      "|2010-12-02|   null|         20855|\n",
      "|2010-12-03|   null|         11548|\n",
      "|2010-12-05|   null|         16394|\n",
      "|2010-12-06|   null|         16095|\n",
      "|2010-12-07|   null|         19351|\n",
      "|2010-12-08|   null|         21275|\n",
      "|2010-12-09|   null|         16904|\n",
      "|2010-12-10|   null|         15388|\n",
      "|2010-12-12|   null|         10561|\n",
      "|2010-12-13|   null|         15234|\n",
      "|2010-12-14|   null|         17108|\n",
      "|2010-12-15|   null|         18169|\n",
      "|2010-12-16|   null|         29482|\n",
      "|2010-12-17|   null|         10517|\n",
      "|2010-12-19|   null|          3735|\n",
      "|2010-12-20|   null|         12617|\n",
      "|2010-12-21|   null|         10888|\n",
      "|2010-12-22|   null|          3053|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.cube(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    "    .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    "    .orderBy(\"Date\")\n",
    "rolledUpDF.where(expr(\"Date IS NULL\")).show()    # null 값을 가진 로우는 전체 날짜의 합계 확인\n",
    "rolledUpDF.where(expr(\"Country IS NULL\")).show() # 모두 null인 로우는 두 컬럼에 속한 레코드의 전체 합계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 그룹화 메타 데이터\n",
    "+ 결과 데이터셋의 집계 수준을 명시하는 컬럼을 제공\n",
    "```\n",
    "3: 가장 높은 계층의 집계 결과에서 나타남\n",
    "2: 개별 재고 코드의 모든 집계 결과에서 나타남\n",
    "1: 구매한 물품에 관계없이 customerId를 기반으로 총 수량을 제공\n",
    "0: customerId와 stockCode별 조합에 따라 총 수량을 제공\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+-------------+\n",
      "|customerId|stockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|      null|     null|            3|      4906888|\n",
      "+----------+---------+-------------+-------------+\n",
      "\n",
      "+----------+---------+-------------+-------------+\n",
      "|customerId|stockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|      null|    21755|            2|         1942|\n",
      "|      null|    82482|            2|         8043|\n",
      "|      null|    21034|            2|         1872|\n",
      "+----------+---------+-------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+---------+-------------+-------------+\n",
      "|customerId|stockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|     14506|     null|            1|          730|\n",
      "|     14850|     null|            1|          285|\n",
      "|     14896|     null|            1|          279|\n",
      "+----------+---------+-------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+---------+-------------+-------------+\n",
      "|customerId|stockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|     14688|   84997B|            0|           12|\n",
      "|     14688|    21929|            0|           10|\n",
      "|     13767|    22730|            0|          108|\n",
      "+----------+---------+-------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import grouping_id, sum, expr\n",
    "\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\\\n",
    "    .orderBy(col(\"grouping_id()\").desc())\\\n",
    "    .where(\"grouping_id() = 3\").show(3)\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\\\n",
    "    .orderBy(col(\"grouping_id()\").desc())\\\n",
    "    .where(\"grouping_id() = 2\").show(3)\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\\\n",
    "    .orderBy(col(\"grouping_id()\").desc())\\\n",
    "    .where(\"grouping_id() = 1\").show(3)\n",
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\\\n",
    "    .orderBy(col(\"grouping_id()\").desc())\\\n",
    "    .where(\"grouping_id() = 0\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 피벗\n",
    "+ 로우를 컬럼으로 변환할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+\n",
      "|      date|USA_sum(CAST(Quantity AS BIGINT))|\n",
      "+----------+---------------------------------+\n",
      "|2011-12-06|                             null|\n",
      "|2011-12-09|                             null|\n",
      "|2011-12-08|                             -196|\n",
      "|2011-12-07|                             null|\n",
      "+----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivoted.columns\n",
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\", \"USA_sum(CAST(Quantity AS BIGINT))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 사용자 정의 집계 함수\n",
    "+ User Defined Aggregation Function, UDAF\n",
    "+ UDAF를 생성하려면 기본 클래스인 UserDefinedAggregateFunction을 상속\n",
    "+ UDAF는 현재 스칼라와 자바로만 사용할 수 있음(ver 2.3)\n",
    "```\n",
    "inputSchema: UDAF 입력 파라미터의 스키마를 StructType으로 정의 \n",
    "bufferSchema: UDAF 중간 결과의 스키마를 StructType으로 정의\n",
    "dataType: 반환될 값의 DataType을 정의\n",
    "deterministic: UDAF가 동일한 입력값에 대해 항상 동일한 결과를 반환하는지 불리언값으로 정의\n",
    "initialize: 집계용 버퍼의 값을 초기화하는 로직을 정의\n",
    "update: 입력받은 로우를 기바느로 내부 버퍼를 업데이트하는 로직을 정의\n",
    "merge: 두 개의 집계용 버퍼를 병합하는 로직을 정의\n",
    "evaluate: 집계의 최종 결과를 생성하는 로직을 정의\n",
    "```\n",
    "\n",
    "※ Efficient UD(A)Fs with PySpark https://www.inovex.de/blog/efficient-udafs-with-pyspark/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
