{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. 데이터 전처리 및 피처 엔지니어링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.1 사용 목적에 따라 모델 서식 지정하기\n",
    "+ 데이터를 Double Type의 컬럼으로 가져와서 레이블을 표시, Vector 타입의 컬럼을 사용해서 특징을 나타냄\n",
    "+ 그래프 분석은 정점과 에지가 각각 DataFrame으로 구성되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션 생성\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"feature engineering examples\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"../BookSamples/data/retail-data/by-day/*.csv\")\\\n",
    "    .coalesce(5)\\\n",
    "    .where(\"Description IS NOT NULL\")\n",
    "    # 널 값을 꼭 걸러내야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeIntDF = spark.read.parquet(\"../BookSamples/data/simple-ml-integers/\")\n",
    "simpleDF = spark.read.json(\"../BookSamples/data/simple-ml/\")\n",
    "scaleDF = spark.read.parquet(\"../BookSamples/data/simple-ml-scaling/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   4|   5|   6|\n",
      "|   7|   8|   9|\n",
      "|   1|   2|   3|\n",
      "+----+----+----+\n",
      "\n",
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|  red|good|    45| 38.97187133755819|\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     1| 38.97187133755819|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  1|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fakeIntDF.show()\n",
    "simpleDF.show()\n",
    "scaleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.2 변환자\n",
    "+ 변환자는 다양한 방식으로 원시 데이터를 변환시키는 함수\n",
    "+ 새로운 상호작용 변수를 생성하거나 컬럼을 정규화, 모델 입력을 위해 Double형 변환 기능 등을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         Description|               words|\n",
      "+--------------------+--------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|\n",
      "|HOT WATER BOTTLE ...|[hot, water, bott...|\n",
      "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|\n",
      "|ROSE CARAVAN DOOR...|[rose, caravan, d...|\n",
      "|GINGHAM HEART  DO...|[gingham, heart, ...|\n",
      "|STORAGE TIN VINTA...|[storage, tin, vi...|\n",
      "|SET OF 4 KNICK KN...|[set, of, 4, knic...|\n",
      "|      POPCORN HOLDER|   [popcorn, holder]|\n",
      "|GROW A FLYTRAP OR...|[grow, a, flytrap...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer(inputCol=\"Description\", outputCol=\"words\")\n",
    "tkn.transform(sales.select(\"Description\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.3 전처리 추정자\n",
    "+ 수행하려는 변환이 입력 컬럼에 대한 데이터 또는 정보로 초기화되어야 할 때 사용\n",
    "    + StandardScaler는 평균과 분산 파라미터가 필요하므로 이를 추정자가 처리함\n",
    "+ inputCol, outputCol 지정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+\n",
      "| id|      features|      scaledFeatures|\n",
      "+---+--------------+--------------------+\n",
      "|  0|[1.0,0.1,-1.0]|[1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|[2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|[1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|[2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|[3.58568582800318...|\n",
      "+---+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "ss = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "ss.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.4 고수준 변환자\n",
    "+ 비즈니스 문제에 집중하기 위해서는 가능한 한 최상위 수준의 변환자 사용을 지향해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.4.1 RFormula\n",
    "+ R에서 빌려옴\n",
    "+ 데이터값은 숫자형 또는 범주형이 되고, 문자열에서 값을 추출하는 등의 조직을 할 필요가 없음\n",
    "+ 기본 연산자\n",
    "    + ~\n",
    "        + 함수에서 타깃과 항을 분리\n",
    "    + \\+\n",
    "        + +0는 y 절편 제거를 의미\n",
    "    + -\n",
    "        + 삭제기호, -1는 y 절편 제거를 의미 +0과 결과가 같음\n",
    "    + :\n",
    "        + 상호작용(수치형 값이나 이진화된 범주 값에 대한 곱셈)\n",
    "    + .\n",
    "        + 타깃 / 종속변수를 제외한 모든 컬럼\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])\n",
      "(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])\n",
      "(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])\n",
      "(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])\n",
      "(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])\n",
      "(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervise = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "for item in supervise.fit(simpleDF).transform(simpleDF).toPandas()[\"features\"] :\n",
    "    print(item) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.4.2 SQL 변환자\n",
    "+ 테이블 이름 대신 THIS 키워드 사용\n",
    "+ DataFrame 조작을 전처리 단계로 공식적으로 코딩하거나 하이퍼파이미터 튜닝 시 특징에 서로 다른 SQL 식을 적용하고자 할 때 SQLTransformer를 사용하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|         1721|     119|   18180.0|\n",
      "|         1070|     107|   12782.0|\n",
      "|          701|      59|   17402.0|\n",
      "|          478|      35|   16642.0|\n",
      "|          477|      28|   16811.0|\n",
      "|          986|      71|   15053.0|\n",
      "|         1419|      50|   12913.0|\n",
      "|          445|      43|   12628.0|\n",
      "|         4505|     183|   14401.0|\n",
      "|          271|      20|   16851.0|\n",
      "|        63012|    1076|   17511.0|\n",
      "|         1032|     128|   18044.0|\n",
      "|         3839|     166|   18198.0|\n",
      "|         5497|     342|   13001.0|\n",
      "|         1408|     109|   16379.0|\n",
      "|         1544|     612|   13230.0|\n",
      "|         3316|     742|   17757.0|\n",
      "|        32324|     201|   17404.0|\n",
      "|         5458|     284|   16705.0|\n",
      "|         2531|     244|   12957.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "    .setStatement(\"\"\"\n",
    "        SELECT sum(Quantity), count(*), CustomerID\n",
    "        FROM __THIS__\n",
    "        GROUP BY CustomerID\n",
    "    \"\"\")\n",
    "\n",
    "basicTransformation.transform(sales).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.4.3 백터 조합기 \n",
    "+ 모든 특징을 하나의 큰 벡터로 연결하여 추정자에 전달하는 기능 제공\n",
    "+ 일반적으로 파이프라인 마지막 단계에서 사용됨\n",
    "+ 다양한 변환자를 사용하여 여러 가지 조작을 수행하고 그에 대한 모든 결과를 모아야 하는 경우 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_3f282ed847e1__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.5 연속형 특징 처리하기 \n",
    "+ 일반적으로 사용되는 2 개의 변환자\n",
    "    + 버켓팅이라는 프로세스를 통한 연속횽 -> 범주형 변환\n",
    "    + 스케일링 및 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.5.1 버켓팅\n",
    "\n",
    "+ Bucketizer를 사용하면 주어진 연속형 특징을 지정한 버켓으로 분할함\n",
    "+ 체중이라는 컬럼을 과체중, 평균, 저체중의 세 가지 버켓으로 나누어 활용할 때 유용\n",
    "+ 세 가지 요구사항\n",
    "    + 분할 배열의 최솟값은 DataFrame의 최솟값보다 작음\n",
    "    + 분할 배열의 최댓값은 DataFrame의 최댓값보다 금\n",
    "    + 분할 배열은 최소 세 개 이상의 값을 지정해서 두 개 이상의 버켓을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|  id|Bucketizer_1728fc3ec134__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            0.0|\n",
      "| 2.0|                            0.0|\n",
      "| 3.0|                            0.0|\n",
      "| 4.0|                            0.0|\n",
      "| 5.0|                            1.0|\n",
      "| 6.0|                            1.0|\n",
      "| 7.0|                            1.0|\n",
      "| 8.0|                            1.0|\n",
      "| 9.0|                            1.0|\n",
      "|10.0|                            2.0|\n",
      "|11.0|                            2.0|\n",
      "|12.0|                            2.0|\n",
      "|13.0|                            2.0|\n",
      "|14.0|                            2.0|\n",
      "|15.0|                            2.0|\n",
      "|16.0|                            2.0|\n",
      "|17.0|                            2.0|\n",
      "|18.0|                            2.0|\n",
      "|19.0|                            2.0|\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 하드 코딩된 값으로 분할 \"\"\"\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|result|\n",
      "+----+------+\n",
      "| 0.0|   0.0|\n",
      "| 1.0|   0.0|\n",
      "| 2.0|   0.0|\n",
      "| 3.0|   1.0|\n",
      "| 4.0|   1.0|\n",
      "| 5.0|   1.0|\n",
      "| 6.0|   1.0|\n",
      "| 7.0|   2.0|\n",
      "| 8.0|   2.0|\n",
      "| 9.0|   2.0|\n",
      "|10.0|   2.0|\n",
      "|11.0|   2.0|\n",
      "|12.0|   3.0|\n",
      "|13.0|   3.0|\n",
      "|14.0|   3.0|\n",
      "|15.0|   4.0|\n",
      "|16.0|   4.0|\n",
      "|17.0|   4.0|\n",
      "|18.0|   4.0|\n",
      "|19.0|   4.0|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 분위수 기준 분할 \"\"\"\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "# 5등분\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5)\\\n",
    "    .setInputCol(\"id\")\\\n",
    "    .setOutputCol(\"result\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 이 외 지역성 기반 해싱(locality sensitivityhashing, LSH)과 같은 고급 기법도 제공함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.5.2 스케일링과 정규화\n",
    "+ MLlib에서는 항상 Vector 타입의 컬럼에서 이 작업이 수행됨\n",
    "+ 주어진 컬럼의 모든 로우를 조사한 다음 해당 벡터의 모든 차원을 고유한 파라미터로 처리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler\n",
    "+ 평균 0, 표준편차 1인 분포를 갖도록 데이터를 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+\n",
      "| id|      features|      scaledFeatures|\n",
      "+---+--------------+--------------------+\n",
      "|  0|[1.0,0.1,-1.0]|[1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|[2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|[1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|[2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|[3.58568582800318...|\n",
      "+---+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "sScaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaledFeatures\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMaxScaler\n",
    "+ 모든 값을 0에서 1사이로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------------+\n",
      "| id|      features|  scaledFeatures|\n",
      "+---+--------------+----------------+\n",
      "|  0|[1.0,0.1,-1.0]|   [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|   [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|   [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|   [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|[10.0,10.0,10.0]|\n",
      "+---+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\").setOutputCol(\"scaledFeatures\")\n",
    "minMax.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxAbsScaler\n",
    "+ 모든 값을 -1에서 1사이로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MaxAbsScaler_6c7f08658eb3__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                    [1.0,1.0,1.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "maScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElementWiseProduct\n",
    "\n",
    "+ 벡터의 각 값을 임의의 값으로 조정할 수 있을 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------+\n",
      "| id|      features|   scaledFeatures|\n",
      "+---+--------------+-----------------+\n",
      "|  0|[1.0,0.1,-1.0]| [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]| [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]| [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]| [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|[30.0,151.5,60.0]|\n",
      "+---+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0) # 10배, 15배, 20배로 조절\n",
    "scalingUp = ElementwiseProduct()\\\n",
    "    .setScalingVec(scaleUpVec)\\\n",
    "    .setInputCol(\"features\")\\\n",
    "    .setOutputCol(\"scaledFeatures\")\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.6  범주형 특징 처리하기\n",
    "\n",
    "### 25.6.1 StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- lab: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: double (nullable = true)\n",
      " |-- labelInd: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()\n",
    "idxRes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "|  red| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    45| 38.97187133755819|     3.0|\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- lab: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: double (nullable = true)\n",
      " |-- valueInd: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 숫자형 컬럼은 자동 문자열로 변환되어 처리됨 \"\"\"\n",
    "\n",
    "valIndxr = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndxr.fit(simpleDF).transform(simpleDF).show()\n",
    "valIndxr.fit(simpleDF).transform(simpleDF).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexer_22c3e7286a28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 유효하지 않은 값에 대해 오류를 출력하거나 로우를 건너뛰고 처리할 수 있음 \"\"\"\n",
    "\n",
    "valIndxr.setHandleInvalid(\"skip\")\n",
    "valIndxr.fit(simpleDF).setHandleInvalid(\"skip\") ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.6.2 색인된 값을 텍스트로 변환하기\n",
    "+ 예측 결과(색인)를 원래 범주로 다시 변환하는 데 유용함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+------------+\n",
      "|color| lab|value1|            value2|labelInd|labelReverse|\n",
      "+-----+----+------+------------------+--------+------------+\n",
      "|green|good|     1|14.386294994851129|     1.0|        good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|         bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|         bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|        good|\n",
      "|green|good|    12|14.386294994851129|     1.0|        good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|         bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|        good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|         bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|         bad|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|         bad|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|        good|\n",
      "|green|good|     1|14.386294994851129|     1.0|        good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|         bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|         bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|        good|\n",
      "|green|good|    12|14.386294994851129|     1.0|        good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|         bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|        good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|         bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|         bad|\n",
      "+-----+----+------+------------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "lblReverse = IndexToString().setInputCol(\"labelInd\").setOutputCol(\"labelReverse\")\n",
    "lblReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.6.3 벡터 인덱싱하기\n",
    "\n",
    "+ 범주형 변수를 대상으로 하는 유용한 도구\n",
    "+ 벡터 내에 존재하는 범주형 데이터를 자동으로 찾아서 0부터 시작하는 카테고리 색인을 사용하여 범주형 특징으로 변환함\n",
    "+ 연속형 변수가 반복된 값이 많아서 고유한 갑시 너무 적으면 의도하지 않게 범주형 변수로 잘못 변환될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "idxIn = spark.createDataFrame([\n",
    "    (Vectors.dense(1, 2, 3), 1),\n",
    "    (Vectors.dense(2, 5, 6), 2),\n",
    "    (Vectors.dense(1, 8, 9), 3)\n",
    "]).toDF(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|     features|label|        idxed|\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|    1|[0.0,0.0,0.0]|\n",
      "|[2.0,5.0,6.0]|    2|[1.0,1.0,1.0]|\n",
      "|[1.0,8.0,9.0]|    3|[0.0,2.0,2.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indxr = VectorIndexer()\\\n",
    "    .setInputCol(\"features\")\\\n",
    "    .setOutputCol(\"idxed\")\\\n",
    "    .setMaxCategories(3)\n",
    "\n",
    "indxr.fit(idxIn).transform(idxIn).show() # 요소가 3개 미만인 모든 컬럼이 인덱싱 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.6.4 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------+\n",
      "|color|colorInd|      colorOH|\n",
      "+-----+--------+-------------+\n",
      "|green|     1.0|(2,[1],[1.0])|\n",
      "| blue|     2.0|    (2,[],[])|\n",
      "| blue|     2.0|    (2,[],[])|\n",
      "|green|     1.0|(2,[1],[1.0])|\n",
      "|green|     1.0|(2,[1],[1.0])|\n",
      "|green|     1.0|(2,[1],[1.0])|\n",
      "|  red|     0.0|(2,[0],[1.0])|\n",
      "|  red|     0.0|(2,[0],[1.0])|\n",
      "|  red|     0.0|(2,[0],[1.0])|\n",
      "|  red|     0.0|(2,[0],[1.0])|\n",
      "|  red|     0.0|(2,[0],[1.0])|\n",
      "|green|     1.0|(2,[1],[1.0])|\n",
      "| blue|     2.0|    (2,[],[])|\n",
      "| blue|     2.0|    (2,[],[])|\n",
      "|green|     1.0|(2,[1],[1.0])|\n",
      "|green|     1.0|(2,[1],[1.0])|\n",
      "|green|     1.0|(2,[1],[1.0])|\n",
      "|  red|     0.0|(2,[0],[1.0])|\n",
      "|  red|     0.0|(2,[0],[1.0])|\n",
      "|  red|     0.0|(2,[0],[1.0])|\n",
      "+-----+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\").setOutputCol(\"colorOH\")\n",
    "ohe.transform(colorLab).show() # 튜플의 의미는 잘 모르겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.7 텍스트 데이터 변환자 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.7.1 텍스트 토큰화하기\n",
    "+ 자연어를 토큰 또는 개별 단어 목록으로 변환하는 프로세스\n",
    "+ 가장 쉬운 방법은 Tokenizer 클래스 사용\n",
    "+ RegexTokenizer를 이용하면 공백뿐만 아니라 정규 표현식을 이용한 Tokenizer를 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, , water, transfer, tattoos]      |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, , doorstop, red]         |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 정규식 이용 \"\"\"\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "rt = RegexTokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\\\n",
    "    .setPattern(\" \")\\\n",
    "    .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------+\n",
      "|Description                        |DescOut           |\n",
      "+-----------------------------------+------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[ ,  ]            |\n",
      "|DOUGHNUT LIP GLOSS                 |[ ,  ,  ]         |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[ ,  ,  ,  ]      |\n",
      "|BLUE HARMONICA IN BOX              |[ ,  ,  ,  ]      |\n",
      "|GUMBALL COAT RACK                  |[ ,  ]            |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[ ,  ,  ,  ,  ]   |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[ ,  ,  ]         |\n",
      "|CAMOUFLAGE LED TORCH               |[ ,  ]            |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[ ,  ,  ,  ,  ]   |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[ ,  ,  ,  ]      |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[ ,  ,  ,  ]      |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[ ,  ,  ,  ]      |\n",
      "|ROSE CARAVAN DOORSTOP              |[ ,  ]            |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[ ,  ,  ,  ]      |\n",
      "|STORAGE TIN VINTAGE LEAF           |[ ,  ,  ]         |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[ ,  ,  ,  ,  ,  ]|\n",
      "|POPCORN HOLDER                     |[ ]               |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[ ,  ,  ,  ,  ,  ]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[ ,  ,  ,  ,  ]   |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[ ,  ,  ,  ,  ]   |\n",
      "+-----------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 사전에 제시된 패턴에 매칭하는 값을 출력(setGaps) \"\"\"\n",
    "\n",
    "rt = RegexTokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\\\n",
    "    .setPattern(\" \")\\\n",
    "    .setGaps(False)\\\n",
    "    .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.7.2 일반적인 단어 제거하기\n",
    "\n",
    "+ the, and, but 등 빈번히 발견되는 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|             DescOut|           DescStops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|[rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|[doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|[12, message, car...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|[blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|[gumball, coat, r...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|[skulls, , water,...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|[feltcraft, girl,...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|[camouflage, led,...|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|[white, skull, ho...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|[english, rose, h...|\n",
      "|HOT WATER BOTTLE ...|[hot, water, bott...|[hot, water, bott...|\n",
      "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|[scottie, dog, ho...|\n",
      "|ROSE CARAVAN DOOR...|[rose, caravan, d...|[rose, caravan, d...|\n",
      "|GINGHAM HEART  DO...|[gingham, heart, ...|[gingham, heart, ...|\n",
      "|STORAGE TIN VINTA...|[storage, tin, vi...|[storage, tin, vi...|\n",
      "|SET OF 4 KNICK KN...|[set, of, 4, knic...|[set, 4, knick, k...|\n",
      "|      POPCORN HOLDER|   [popcorn, holder]|   [popcorn, holder]|\n",
      "|GROW A FLYTRAP OR...|[grow, a, flytrap...|[grow, flytrap, s...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|[airline, bag, vi...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|[airline, bag, vi...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "    .setStopWords(englishStopWords)\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"DescStops\")\n",
    "stops.transform(tokenized).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.7.3 단어 조합만들기 (n-gram)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------+\n",
      "|DescOut                              |NGram_abda004b9a63__output           |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit, night, light]               |\n",
      "|[doughnut, lip, gloss]               |[doughnut, lip, gloss]               |\n",
      "|[12, message, cards, with, envelopes]|[12, message, cards, with, envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue, harmonica, in, box]           |\n",
      "|[gumball, coat, rack]                |[gumball, coat, rack]                |\n",
      "|[skulls, , water, transfer, tattoos] |[skulls, , water, transfer, tattoos] |\n",
      "|[feltcraft, girl, amelie, kit]       |[feltcraft, girl, amelie, kit]       |\n",
      "|[camouflage, led, torch]             |[camouflage, led, torch]             |\n",
      "|[white, skull, hot, water, bottle]   |[white, skull, hot, water, bottle]   |\n",
      "|[english, rose, hot, water, bottle]  |[english, rose, hot, water, bottle]  |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|DescOut                              |NGram_805cf48adbee__output                             |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit night, night light]                            |\n",
      "|[doughnut, lip, gloss]               |[doughnut lip, lip gloss]                              |\n",
      "|[12, message, cards, with, envelopes]|[12 message, message cards, cards with, with envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue harmonica, harmonica in, in box]                 |\n",
      "|[gumball, coat, rack]                |[gumball coat, coat rack]                              |\n",
      "|[skulls, , water, transfer, tattoos] |[skulls ,  water, water transfer, transfer tattoos]    |\n",
      "|[feltcraft, girl, amelie, kit]       |[feltcraft girl, girl amelie, amelie kit]              |\n",
      "|[camouflage, led, torch]             |[camouflage led, led torch]                            |\n",
      "|[white, skull, hot, water, bottle]   |[white skull, skull hot, hot water, water bottle]      |\n",
      "|[english, rose, hot, water, bottle]  |[english rose, rose hot, hot water, water bottle]      |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(10, False)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.7.4 단어를 숫자로 변환하기\n",
    "\n",
    "+ CountVectorizer 토큰화된 데이터에서 작동함\n",
    "    + 모든 문서에서 단어 집합을 찾은 다음 문서별로 해당 단어의 출현 빈도를 계산\n",
    "    + 변환 과정에서 DataFrame 컬럼의 각 로우에서 주어진 단어의 발생 빈도를 계산하고 해당 로우에 포함된 용어를 벡터 형태로 출력함\n",
    "+ 모든 로우를 문서(document), 모든 단어를 용어(term), 모든 용어의 집합을 어휘집(vocabulary)로 취급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "|Description                    |DescOut                              |countVec                                     |\n",
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |(500,[150,185,212],[1.0,1.0,1.0])            |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |(500,[462,463,491],[1.0,1.0,1.0])            |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|(500,[35,41,166],[1.0,1.0,1.0])              |\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |(500,[10,16,36,352],[1.0,1.0,1.0,1.0])       |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |(500,[228,281,407],[1.0,1.0,1.0])            |\n",
      "|SKULLS  WATER TRANSFER TATTOOS |[skulls, , water, transfer, tattoos] |(500,[11,40,133],[1.0,1.0,1.0])              |\n",
      "|FELTCRAFT GIRL AMELIE KIT      |[feltcraft, girl, amelie, kit]       |(500,[60,64,69],[1.0,1.0,1.0])               |\n",
      "|CAMOUFLAGE LED TORCH           |[camouflage, led, torch]             |(500,[263],[1.0])                            |\n",
      "|WHITE SKULL HOT WATER BOTTLE   |[white, skull, hot, water, bottle]   |(500,[15,34,39,40,118],[1.0,1.0,1.0,1.0,1.0])|\n",
      "|ENGLISH ROSE HOT WATER BOTTLE  |[english, rose, hot, water, bottle]  |(500,[34,39,40,46,169],[1.0,1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'총 어휘 크기, 어휘에 포함된 단어 색인, 특정 단어의 출현 빈도 순으로 출력됨'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"countVec\")\\\n",
    "    .setVocabSize(500)\\\n",
    "    .setMinTF(1)\\\n",
    "    .setMinDF(2)\n",
    "\n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(10, False)\n",
    "\n",
    "\"총 어휘 크기, 어휘에 포함된 단어 색인, 특정 단어의 출현 빈도 순으로 출력됨\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|DescOut                                |\n",
      "+---------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[red, floral, feltcraft, shoulder, bag]|\n",
      "|[alarm, clock, bakelike, red]          |\n",
      "|[pin, cushion, babushka, red]          |\n",
      "|[red, retrospot, mini, cases]          |\n",
      "|[red, kitchen, scales]                 |\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[large, red, babushka, notebook]       |\n",
      "|[red, retrospot, oven, glove]          |\n",
      "|[red, retrospot, plate]                |\n",
      "+---------------------------------------+\n",
      "\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|DescOut                                |TFOut                                                   |IDFOut                                                                                                              |\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[3372,4291,4370,6594,9160],[1.0,1.0,1.0,1.0,1.0])|(10000,[3372,4291,4370,6594,9160],[1.2992829841302609,0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[red, floral, feltcraft, shoulder, bag]|(10000,[155,1152,4291,5981,6756],[1.0,1.0,1.0,1.0,1.0]) |(10000,[155,1152,4291,5981,6756],[0.0,0.0,0.0,0.0,0.0])                                                             |\n",
      "|[alarm, clock, bakelike, red]          |(10000,[4291,4852,4995,9668],[1.0,1.0,1.0,1.0])         |(10000,[4291,4852,4995,9668],[0.0,0.0,0.0,0.0])                                                                     |\n",
      "|[pin, cushion, babushka, red]          |(10000,[4291,5111,5673,7153],[1.0,1.0,1.0,1.0])         |(10000,[4291,5111,5673,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, mini, cases]          |(10000,[547,1576,2591,4291],[1.0,1.0,1.0,1.0])          |(10000,[547,1576,2591,4291],[0.0,0.0,1.0116009116784799,0.0])                                                       |\n",
      "|[red, kitchen, scales]                 |(10000,[3461,4291,6214],[1.0,1.0,1.0])                  |(10000,[3461,4291,6214],[0.0,0.0,0.0])                                                                              |\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[3372,4291,4370,6594,9160],[1.0,1.0,1.0,1.0,1.0])|(10000,[3372,4291,4370,6594,9160],[1.2992829841302609,0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[large, red, babushka, notebook]       |(10000,[2782,2787,4291,7153],[1.0,1.0,1.0,1.0])         |(10000,[2782,2787,4291,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, oven, glove]          |(10000,[302,2591,4291,8242],[1.0,1.0,1.0,1.0])          |(10000,[302,2591,4291,8242],[0.0,1.0116009116784799,0.0,0.0])                                                       |\n",
      "|[red, retrospot, plate]                |(10000,[2591,4291,4456],[1.0,1.0,1.0])                  |(10000,[2591,4291,4456],[1.0116009116784799,0.0,0.0])                                                               |\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Red가 포함된 백터\n",
    "tfIdfIn = tokenized\\\n",
    "    .where(\"array_contains(DescOut, 'red')\")\\\n",
    "    .select(\"DescOut\")\\\n",
    "    .limit(10)\n",
    "tfIdfIn.show(10, False)\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "tf = HashingTF()\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"TFOut\")\\\n",
    "    .setNumFeatures(10000)\n",
    "\n",
    "idf = IDF()\\\n",
    "    .setInputCol(\"TFOut\")\\\n",
    "    .setOutputCol(\"IDFOut\")\\\n",
    "    .setMinDocFreq(2)\n",
    "\n",
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.7.5 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.026425288617610933,-0.015249719843268394,-0.000582711398601532]\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.026020058962915624,-0.03844750779015677,-0.03598763979971409]\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.03285085335373879,0.03471647389233112,-0.028486729227006437]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# 각 로우는 문장 또는 문서의 단어 주머니\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# 단어를 백터에 매핑\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(f\"Text: [{', '.join(text)}] => \\nVector: {str(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.8 특징 조작하기\n",
    "+ feature space를 조작하는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.8.1 주성분 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_a4324eb11acf__output                  |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060149758]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.8.2 상호작용\n",
    "+ Rformual를 사용하여 생성하는 것을 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.8.3 다항식 전개\n",
    "+ 모든 입력 컬럼의 상호작용 변수를 생성하는 데 사용됨\n",
    "+ 특정 특징 간의 상호작용을 검토해보고는 싶지만 정확히 무엇을 사용해야 할지 확신하지 못할 때 유용하게 사용할 수 있음\n",
    "+ 특징 공간을 상당히 크게 확장시킬 수 있어서 계산 비용이 많고, 과적합을 초래할 수 있으므로 주의 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------------------------------------+\n",
      "| id|      features|PolynomialExpansion_5b924a9df1a2__output|\n",
      "+---+--------------+----------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
      "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
      "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
      "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
      "|  1|[3.0,10.1,3.0]|                    [3.0,9.0,10.1,30....|\n",
      "+---+--------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
    "pe.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.9 특징 선택\n",
    "\n",
    "+ 과적합을 방지할 수 있도록 ChiSqSelector와 같은 간단한 옵션을 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.9.1 ChiSqSelector\n",
    "\n",
    "+ 통계적 검정을 활용하여 예측하려는 레이블과 독립적이지 않은 특징을 식별하고 관련 없는 특징을 삭제함\n",
    "+ 카이제곱 검정을 기반으로 동작함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------------+\n",
      "|            countVec|ChiSqSelector_4be64376a557__output|\n",
      "+--------------------+----------------------------------+\n",
      "|(500,[150,185,212...|                         (2,[],[])|\n",
      "|(500,[462,463,491...|                         (2,[],[])|\n",
      "|(500,[35,41,166],...|                         (2,[],[])|\n",
      "|(500,[10,16,36,35...|                         (2,[],[])|\n",
      "|(500,[228,281,407...|                         (2,[],[])|\n",
      "|(500,[11,40,133],...|                         (2,[],[])|\n",
      "|(500,[60,64,69],[...|                         (2,[],[])|\n",
      "|   (500,[263],[1.0])|                         (2,[],[])|\n",
      "|(500,[15,34,39,40...|                         (2,[],[])|\n",
      "|(500,[34,39,40,46...|                         (2,[],[])|\n",
      "|(500,[34,39,40,14...|                         (2,[],[])|\n",
      "|(500,[34,39,40,14...|                         (2,[],[])|\n",
      "|(500,[46,297],[1....|                         (2,[],[])|\n",
      "|(500,[3,4,11,143,...|                         (2,[],[])|\n",
      "|(500,[6,45,109,16...|                         (2,[],[])|\n",
      "|(500,[0,1,49,70,3...|               (2,[0,1],[1.0,1.0])|\n",
      "|(500,[21,296],[1....|                         (2,[],[])|\n",
      "|(500,[36,45,378],...|                         (2,[],[])|\n",
      "|(500,[2,6,328],[1...|                         (2,[],[])|\n",
      "|(500,[0,2,6,328,4...|                     (2,[0],[1.0])|\n",
      "+--------------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn\\\n",
    "    .transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    "    .where(\"CustomerId IS NOT NULL\")\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    "    .where(\"CustomerId IS NOT NULL\")\n",
    "chisq = ChiSqSelector()\\\n",
    "    .setFeaturesCol(\"countVec\")\\\n",
    "    .setLabelCol(\"CustomerId\")\\\n",
    "    .setNumTopFeatures(2)\n",
    "chisq.fit(prechi).transform(prechi)\\\n",
    "    .drop(\"customerId\", \"Description\", \"DescOut\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.10 고급 주제\n",
    "+ 변환자 저장하기\n",
    "+ 사용자 정의 변환자 작성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.10.1 변환자 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 저장하기 \"\"\"\n",
    "fittedPCA = pca.fit(scaleDF)\n",
    "fittedPCA.write().overwrite().save(\"/tmp/fittedPCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------+\n",
      "| id|      features|PCA_a4324eb11acf__output|\n",
      "+---+--------------+------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|    [0.07137194992484...|\n",
      "|  1| [2.0,1.1,1.0]|    [-1.6804946984073...|\n",
      "|  0|[1.0,0.1,-1.0]|    [0.07137194992484...|\n",
      "|  1| [2.0,1.1,1.0]|    [-1.6804946984073...|\n",
      "|  1|[3.0,10.1,3.0]|    [-10.872398139848...|\n",
      "+---+--------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCAModel\n",
    "\n",
    "\"\"\" 불러오기 \"\"\"\n",
    "loadedPCA = PCAModel.load(\"/tmp/fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.10.2 사용자 정의 변환자 작성하기\n",
    "\n",
    "+ 일반적으로 내장 모듈은 효율적으로 실행되도록 최적화되어 있으므로 가능한 한 많이 사용\n",
    "+ 책의 예제는 파이썬 버전이 없으므로 생략"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
