{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23.운영 환경에서의 구조적 스트리밍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.1 내고장성과 체크포인팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 장애복구를 위해 체크포인팅과 WAL 정보를 저장하는 신뢰도 높은 파일 시스템의 경로를 쿼리에 설정\n",
    "+ writeStream의 checkpointLocation 옵션 사용(이 정보가 지워지면 처음부터 해야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val datapath = \"../BookSamples/data/activity-data\"\n",
    "\n",
    "val static = spark.read.json(datapath)\n",
    "\n",
    "val streaming = spark\n",
    "    .readStream\n",
    "    .schema(static.schema)\n",
    "    .option(\"maxFilesPerTrigger\", 10)\n",
    "    .json(datapath)\n",
    "    .groupBy(\"gt\")\n",
    "    .count()\n",
    "\n",
    "val query = streaming\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"./checkpoint\") // check point location\n",
    "    .queryName(\"test_stream\")\n",
    "    .format(\"memory\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.2 애플리케이션 변경하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.2.1 스트리밍 애플리케이션 코드 업데이트하기\n",
    "\n",
    "+ 사용자 정의 함수는 시그니처가 같은 경우에만 코드 변경\n",
    "    + 구조적 스트리밍을 사용 중이라면 새로운 버전의 파싱 함수를 만들어 애플리케이션을 다시 컴파일\n",
    "    + 중대한 변화에는 신규 체크포인트 디렉터리를 지정해야 함\n",
    "        + 새로운 집계 키 추가, 완전한 쿼리 변경 등        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.2.2 스파크 버전 업데이트하기\n",
    "상위 호완성을 지키려 하지만 릴리즈 노트를 확인하고 지원하지 않는 경우 새로운 체크포인트 디텍터리를 지정해 애플리케이션을 다시 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.2.3 애플리케이션의 초기 규모 산정과 재조정하기\n",
    "+ 전반적으로 유입률이 처리율보다 크다면 클러스터나 애플리케이션의 크기를 늘려야 함\n",
    "+ 익스큐터를 제거하거나 애플리케이션에 설정된 자원을 줄인 다음 재시작하는 방식으로 크기를 줄일 수 있음\n",
    "+ partition 숫자 등 새로운 설정을 적용하는 경우 다시 시작이 필요할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.3 메트릭과 모니터링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 가장 기본적인 접근\n",
    "\n",
    "```\n",
    "streamingQuery.status\n",
    "\n",
    "{\n",
    "  \"message\" : \"Waiting for data to arrive\",\n",
    "  \"isDataAvailable\" : false,\n",
    "  \"isTriggerActive\" : false\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 최근 현황\n",
    "\n",
    "```\n",
    "Array({\n",
    "  \"id\" : \"47ed4ee9-e12e-467f-9b29-dfe05abf007f\",\n",
    "  \"runId\" : \"4798f354-6c9f-4095-a2cd-659aa9d01d76\",\n",
    "  \"name\" : \"count_based_device\",\n",
    "  \"timestamp\" : \"2020-06-27T00:41:50.338Z\",\n",
    "  \"batchId\" : 0,\n",
    "  \"numInputRows\" : 780119,\n",
    "  \"processedRowsPerSecond\" : 131932.8598004397,\n",
    "  \"durationMs\" : {\n",
    "    \"addBatch\" : 5383,\n",
    "    \"getBatch\" : 135,\n",
    "    \"getOffset\" : 84,\n",
    "    \"queryPlanning\" : 210,\n",
    "    \"triggerExecution\" : 5912,\n",
    "    \"walCommit\" : 45\n",
    "  },\n",
    "  \"eventTime\" : {\n",
    "    \"avg\" : \"2015-02-24T03:00:44.892Z\",\n",
    "    \"max\" : \"2015-02-24T15:21:45.941Z\",\n",
    "    \"min\" : \"2015-02-22T00:41:40.120Z\",\n",
    "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
    "  },\n",
    "  \"stateOperators\" : [ {\n",
    "    \"numRowsTotal\" : 9,\n",
    "    \"numRowsUpdated\" : 9,\n",
    "    \"memoryUsedB...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 일반적으로 배치 주기, 유입률, 처리율에 대한 시간별 변화를 시각화하는 방식이 유용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23.5 스트리밍 리스터를 사용한 고급 모니터링\n",
    "\n",
    "```\n",
    "// 콘솔에 출력\n",
    "spark.streams.addListener(new StreamingQueryListener() {\n",
    "    override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {\n",
    "        println(\"Query started: \" + queryStarted.id)\n",
    "    }\n",
    "    override def onQueryTerminated(\n",
    "      queryTerminated: QueryTerminatedEvent): Unit = {\n",
    "        println(\"Query terminated: \" + queryTerminated.id)\n",
    "    }\n",
    "    override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {\n",
    "        println(\"Query made progress: \" + queryProgress.progress)\n",
    "    }\n",
    "})\n",
    "\n",
    "\n",
    "// 카프카를 활용하는 방법\n",
    "class KafkaMetrics(servers: String) extends StreamingQueryListener {\n",
    "  val kafkaProperties = new Properties()\n",
    "  kafkaProperties.put(\n",
    "    \"bootstrap.servers\",\n",
    "    servers)\n",
    "  kafkaProperties.put(\n",
    "    \"key.serializer\",\n",
    "    \"kafkashaded.org.apache.kafka.common.serialization.StringSerializer\")\n",
    "  kafkaProperties.put(\n",
    "    \"value.serializer\",\n",
    "    \"kafkashaded.org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "  val producer = new KafkaProducer[String, String](kafkaProperties)\n",
    "\n",
    "  import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "  import org.apache.kafka.clients.producer.KafkaProducer\n",
    "\n",
    "  override def onQueryProgress(event:\n",
    "    StreamingQueryListener.QueryProgressEvent): Unit = {\n",
    "    producer.send(new ProducerRecord(\"streaming-metrics\",\n",
    "      event.progress.json))\n",
    "  }\n",
    "  override def onQueryStarted(event:\n",
    "    StreamingQueryListener.QueryStartedEvent): Unit = {}\n",
    "  override def onQueryTerminated(event:\n",
    "    StreamingQueryListener.QueryTerminatedEvent): Unit = {}\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
