{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@3fc7553d"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이벤트 시간과 상태 기반 처리 \n",
    "+ 이벤트 시간 처리 \n",
    "    + 이벤트 시간 : 데이터에 기록된 시간 기준 처리 \n",
    "    + 처리 시간 : 스트림 처리 시스템이 데이터를 실제로 수신한 시간 \n",
    "+ 상태 기반 처리 \n",
    "    + 중간 처리 정보를 사용하거나 갱신하는 경우에 필요(마이크로 배치 혹은 레코드 단위 처리) \n",
    "    + 스파크는 상태 저장소를 제공 \n",
    "+ 임의적인 상태 기반 처리 \n",
    "    + 상태의 유형, 갱신 방법, 제거 시점에 따라 세밀한 제어가 필요한 경우 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.4 이벤트 시간 처리의 기본  \n",
    "+ Creation_Time: 이벤트가 생성된 시간 \n",
    "+ Arrival_Time: 서버에 도착한 시간 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "activeDataPath = ../BookSamples/data/activity-data\n",
       "static = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n",
       "streaming = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) \n",
    "\n",
    "val activeDataPath = \"../BookSamples/data/activity-data\"\n",
    "val static = spark.read.json(activeDataPath) \n",
    "val streaming = spark.readStream.schema(static.schema).option(\"maxFilesPerTrigger\", 10).json(activeDataPath)\n",
    "\n",
    "static.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.5 이벤트 시간 윈도우 \n",
    "+ 나노세컨드 단위의 유닉스 시간으로 표현됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "withEventTime = [Arrival_Time: bigint, Creation_Time: bigint ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[Arrival_Time: bigint, Creation_Time: bigint ... 9 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{window, col}\n",
    "\n",
    "val withEventTime = streaming.selectExpr(\"*\", \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.5.1 텀블링 윈도우 \n",
    "+ 디버깅 목적으로 결과를 메모리 싱크에 저장함 \n",
    "+ 스트림 처리가 시작되면 SQL로 결과 조회가 가능함 \n",
    "+ windows 컬럼은 struct 타입이며 윈도우 시작과 종료를 의미함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "active = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7f1cefeb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7f1cefeb"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val active = withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\"))\n",
    "    .count() \n",
    "    .writeStream\n",
    "    .queryName(\"events_per_windows2\") \n",
    "    .format(\"memory\") \n",
    "    .outputMode(\"complete\") \n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+\n",
      "|window                                    |count |\n",
      "+------------------------------------------+------+\n",
      "|[2015-02-24 11:50:00, 2015-02-24 12:00:00]|150773|\n",
      "|[2015-02-24 13:00:00, 2015-02-24 13:10:00]|133323|\n",
      "|[2015-02-23 12:30:00, 2015-02-23 12:40:00]|100853|\n",
      "|[2015-02-23 10:20:00, 2015-02-23 10:30:00]|99178 |\n",
      "|[2015-02-24 12:30:00, 2015-02-24 12:40:00]|125679|\n",
      "|[2015-02-24 13:10:00, 2015-02-24 13:20:00]|105494|\n",
      "|[2015-02-23 10:30:00, 2015-02-23 10:40:00]|100443|\n",
      "|[2015-02-23 10:40:00, 2015-02-23 10:50:00]|88681 |\n",
      "|[2015-02-23 13:20:00, 2015-02-23 13:30:00]|106075|\n",
      "|[2015-02-22 00:40:00, 2015-02-22 00:50:00]|35    |\n",
      "|[2015-02-24 11:20:00, 2015-02-24 11:30:00]|113768|\n",
      "|[2015-02-24 12:20:00, 2015-02-24 12:30:00]|133623|\n",
      "|[2015-02-24 14:00:00, 2015-02-24 14:10:00]|150225|\n",
      "|[2015-02-24 14:10:00, 2015-02-24 14:20:00]|169064|\n",
      "|[2015-02-24 13:40:00, 2015-02-24 13:50:00]|132243|\n",
      "|[2015-02-24 13:50:00, 2015-02-24 14:00:00]|96023 |\n",
      "|[2015-02-23 14:30:00, 2015-02-23 14:40:00]|94669 |\n",
      "|[2015-02-23 13:40:00, 2015-02-23 13:50:00]|167565|\n",
      "|[2015-02-24 12:00:00, 2015-02-24 12:10:00]|200133|\n",
      "|[2015-02-23 12:20:00, 2015-02-23 12:30:00]|106291|\n",
      "+------------------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from events_per_windows2\").show(20, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "active.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 슬라이딩 윈도우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "active = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4ad7ef6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4ad7ef6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{window, col} \n",
    "\n",
    "val active = withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\")) \n",
    "    .count() \n",
    "    .writeStream \n",
    "    .queryName(\"events_per_windows\") \n",
    "    .format(\"memory\") \n",
    "    .outputMode(\"complete\") \n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+\n",
      "|window                                    |count |\n",
      "+------------------------------------------+------+\n",
      "|[2015-02-23 14:15:00, 2015-02-23 14:25:00]|107668|\n",
      "|[2015-02-24 11:50:00, 2015-02-24 12:00:00]|150773|\n",
      "|[2015-02-24 13:00:00, 2015-02-24 13:10:00]|133323|\n",
      "|[2015-02-22 00:35:00, 2015-02-22 00:45:00]|35    |\n",
      "|[2015-02-23 12:30:00, 2015-02-23 12:40:00]|100853|\n",
      "|[2015-02-23 10:20:00, 2015-02-23 10:30:00]|99178 |\n",
      "|[2015-02-23 13:25:00, 2015-02-23 13:35:00]|91684 |\n",
      "|[2015-02-24 14:25:00, 2015-02-24 14:35:00]|203945|\n",
      "|[2015-02-23 12:55:00, 2015-02-23 13:05:00]|113953|\n",
      "|[2015-02-22 00:40:00, 2015-02-22 00:50:00]|35    |\n",
      "|[2015-02-23 12:35:00, 2015-02-23 12:45:00]|91221 |\n",
      "|[2015-02-23 13:05:00, 2015-02-23 13:15:00]|205912|\n",
      "|[2015-02-24 11:20:00, 2015-02-24 11:30:00]|113768|\n",
      "|[2015-02-24 13:35:00, 2015-02-24 13:45:00]|174863|\n",
      "|[2015-02-24 14:00:00, 2015-02-24 14:10:00]|150225|\n",
      "|[2015-02-24 12:30:00, 2015-02-24 12:40:00]|125679|\n",
      "|[2015-02-24 13:10:00, 2015-02-24 13:20:00]|105494|\n",
      "|[2015-02-23 10:30:00, 2015-02-23 10:40:00]|100443|\n",
      "|[2015-02-24 11:45:00, 2015-02-24 11:55:00]|138413|\n",
      "|[2015-02-23 10:40:00, 2015-02-23 10:50:00]|88681 |\n",
      "+------------------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from events_per_windows\").show(20, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "active.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.5.2 워터마크로 지연 데이터 제어하기 \n",
    "+ 얼마나 늦게 도착한 데이터까지 받아들일지 기준을 정하지 않았기 때문에 중간 결과 데이터를 영원히 저장하는 문제가 있음 \n",
    "+ 오래된 데이터를 제거하는 데 필요한 워터마크를 반드시 지정해야 함 \n",
    "+ 최대 5시간까지 데이터가 지연된다면? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "active = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@29dac4ad\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@29dac4ad"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val active = withEventTime \n",
    "    .withWatermark(\"event_time\", \"5 hours\") \n",
    "    .groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\")) \n",
    "    .count() \n",
    "    .writeStream\n",
    "    .queryName(\"events_per_windows\") \n",
    "    .format(\"memory\") \n",
    "    .outputMode(\"complete\") \n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active.stop()\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.6 스트림에서 중복 데이터 제거하기 \n",
    "+ 상태 정보가 무한히 커지지 않도록 워터마크를 명시해야 함 \n",
    "+ 예제에서는 중복 이벤트가 같은 식별자와 타임스탬프를 가진다고 가정함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "active = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@20af0964\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@20af0964"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val active = withEventTime\n",
    "    .withWatermark(\"event_time\", \"5 hours\") \n",
    "    .dropDuplicates(\"User\", \"event_time\") \n",
    "    .groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\")) \n",
    "    .count() \n",
    "    .writeStream \n",
    "    .queryName(\"events_per_windows\") \n",
    "    .format(\"memory\") \n",
    "    .outputMode(\"complete\") \n",
    "    .start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----+\n",
      "|window                                    |count|\n",
      "+------------------------------------------+-----+\n",
      "|[2015-02-23 14:15:00, 2015-02-23 14:25:00]|13390|\n",
      "|[2015-02-24 11:50:00, 2015-02-24 12:00:00]|18932|\n",
      "|[2015-02-24 13:00:00, 2015-02-24 13:10:00]|16604|\n",
      "|[2015-02-22 00:35:00, 2015-02-22 00:45:00]|2    |\n",
      "|[2015-02-23 12:30:00, 2015-02-23 12:40:00]|12754|\n",
      "|[2015-02-23 10:20:00, 2015-02-23 10:30:00]|12523|\n",
      "|[2015-02-23 13:25:00, 2015-02-23 13:35:00]|11499|\n",
      "|[2015-02-24 12:30:00, 2015-02-24 12:40:00]|15794|\n",
      "|[2015-02-24 13:10:00, 2015-02-24 13:20:00]|13157|\n",
      "|[2015-02-24 14:25:00, 2015-02-24 14:35:00]|25223|\n",
      "|[2015-02-23 10:30:00, 2015-02-23 10:40:00]|12449|\n",
      "|[2015-02-24 11:45:00, 2015-02-24 11:55:00]|17355|\n",
      "|[2015-02-23 10:40:00, 2015-02-23 10:50:00]|11001|\n",
      "|[2015-02-23 12:55:00, 2015-02-23 13:05:00]|14032|\n",
      "|[2015-02-23 13:20:00, 2015-02-23 13:30:00]|13293|\n",
      "|[2015-02-22 00:40:00, 2015-02-22 00:50:00]|2    |\n",
      "|[2015-02-23 12:35:00, 2015-02-23 12:45:00]|11352|\n",
      "|[2015-02-23 13:05:00, 2015-02-23 13:15:00]|25756|\n",
      "|[2015-02-24 11:20:00, 2015-02-24 11:30:00]|14245|\n",
      "|[2015-02-24 12:20:00, 2015-02-24 12:30:00]|16703|\n",
      "+------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from events_per_windows\").show(20, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "active.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.7 임의적인 상태 기반 처리\n",
    "+ 다음과 같은 처리 가능\n",
    "    + 특정 키의 개수를 기반으로 윈도우 생성\n",
    "    + 특성 시간 범위 안에 일정 개수 이상의 이벤트가 있는 경우 알림 발생시키기\n",
    "    + 결정되지 않은 시간 동안 사용자 세션을 유지하고 향후 분석을 위해 세션 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 두 가지의 유형\n",
    "    + mapGroupsWithState API : 최대 한 개의 로우를 만듬\n",
    "    + flatMapGroupsWithState API : 하나 이상의 로우를 만듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.7.1 타임아웃\n",
    "+ 타임 아웃은 전체 그룹에 대한 전역 파라미터로 동작함\n",
    "+ 타임아웃은 처리 시간(GroupStateTimeout.ProcessingTimeTimeout)이나 이벤트 시간(GroupStateTimeout.EventTimeTimeout) 중 하나가 될 수 있음\n",
    "+ state.hasTimedOut 값이나 values 이터레이터가 비어 있는지 확인하는 방식으로 타임아웃 정보 획득\n",
    "+ 처리시간 기반인 경우 GroupState.setTimeoutDuration 설정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.7.2 출력 모드\n",
    "+ mapGroupsWithState는 update 모드만 지원\n",
    "+ flatMapGroupsWithState 은 update와 append 모드를 사용할 수 있음\n",
    "+ append 모드는 타임아웃 이후에 결과 셋에서 데이터를 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.7.3 mapGroupsWithState\n",
    "+ 필요 요소\n",
    "    + 세 가지 클래스 정의 : 입력 클래스, 상태 클래스, 출력 클래스(선택)\n",
    "    + 키, 이벤트 이터레이터 그리고 이전 상태를 기반으로 상태를 갱신하는 함수\n",
    "    + 타임아웃 파라미터 (22.7.1절)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6240991"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|Arrival_Time |Creation_Time      |Device  |Index|Model |User|gt   |x           |y           |z           |\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|18   |nexus4|g   |stand|3.356934E-4 |-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|66   |nexus4|g   |stand|-0.005722046|0.029083252 |0.005569458 |\n",
      "|1424686735500|1424686733498505625|nexus4_1|99   |nexus4|g   |stand|0.0078125   |-0.017654419|0.010025024 |\n",
      "|1424686735691|1424688581745026978|nexus4_2|145  |nexus4|g   |stand|-3.814697E-4|0.0184021   |-0.013656616|\n",
      "|1424686735890|1424688581945252808|nexus4_2|185  |nexus4|g   |stand|-3.814697E-4|-0.031799316|-0.00831604 |\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|User| count|\n",
      "+----+------+\n",
      "|   a|646829|\n",
      "|   b|729907|\n",
      "|   c|617237|\n",
      "|   d|649961|\n",
      "|   e|768182|\n",
      "|   f|736442|\n",
      "|   g|733387|\n",
      "|   h|618617|\n",
      "|   i|740429|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{first, col}\n",
    "\n",
    "static.groupBy(\"User\").count().sort(\"User\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class InputRow\n",
       "defined class UserState\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "case class InputRow(user:String, timestamp:java.sql.Timestamp, activity:String)\n",
    "case class UserState(user:String,\n",
    "                     var activity:String,\n",
    "                     var start:java.sql.Timestamp,\n",
    "                     var end:java.sql.Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateUserStateWithEvent: (state: UserState, input: InputRow)UserState\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "// 특정 행동을 얼마나 지속하고 있는 지 사용자 상태를 업데이트\n",
    "def updateUserStateWithEvent(state:UserState, input:InputRow):UserState = {\n",
    "    if (Option(input.timestamp).isEmpty){\n",
    "        return state\n",
    "    }\n",
    "    if (state.activity == input.activity){\n",
    "        state.end = input.timestamp\n",
    "        \n",
    "        if (input.timestamp.after(state.end)){\n",
    "            state.end = input.timestamp\n",
    "        }\n",
    "        if (input.timestamp.before(state.start)){\n",
    "            state.start = input.timestamp\n",
    "        }\n",
    "    }\n",
    "    else {\n",
    "        if (input.timestamp.after(state.end)){\n",
    "            state.start = input.timestamp\n",
    "            state.end = input.timestamp\n",
    "            state.activity = input.activity\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateAcrossEvents: (user: String, inputs: Iterator[InputRow], oldState: org.apache.spark.sql.streaming.GroupState[UserState])UserState\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}\n",
    "\n",
    "def updateAcrossEvents(user:String,\n",
    "                       inputs:Iterator[InputRow],\n",
    "                       oldState: GroupState[UserState]):UserState = {\n",
    "    \n",
    "    var state:UserState = if (oldState.exists) oldState.get\n",
    "                          else UserState(user, \"\",\n",
    "                                         new java.sql.Timestamp(6284160000000L), // 초기에 무조건 업데이트 되도록 \n",
    "                                         new java.sql.Timestamp(6284160L))       // 시간시간과 종료시간을 Max로 뒤집음\n",
    "    // 시간 비교를 위해 이전 날짜를 간단하게 지정하고 데이터의 값을 기준으로 즉시 변경\n",
    "    for (input <- inputs) {\n",
    "        state = updateUserStateWithEvent(state, input)\n",
    "        oldState.update(state)\n",
    "    }\n",
    "    state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "active = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@23b33a23\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@23b33a23"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.GroupStateTimeout\n",
    "\n",
    "val active = withEventTime\n",
    "    .selectExpr(\"User as user\", \"cast(Creation_time/1000000000 as timestamp) as timestamp\", \"gt as activity\")\n",
    "    .as[InputRow]\n",
    "    .groupByKey(_.user)\n",
    "    .mapGroupsWithState(GroupStateTimeout.NoTimeout)(updateAcrossEvents)\n",
    "    .writeStream\n",
    "    .queryName(\"events_per_window\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------------------------+--------------------------+\n",
      "|user|activity|start                     |end                       |\n",
      "+----+--------+--------------------------+--------------------------+\n",
      "|a   |bike    |2015-02-23 13:30:15.772492|2015-02-23 13:35:15.285731|\n",
      "|a   |bike    |2015-02-23 13:30:15.787598|2015-02-23 13:35:15.295954|\n",
      "|a   |bike    |2015-02-23 13:30:15.837396|2015-02-23 13:35:15.351252|\n",
      "|a   |bike    |2015-02-23 13:30:15.792481|2015-02-23 14:06:02.857617|\n",
      "|a   |bike    |2015-02-23 13:30:16.230041|2015-02-23 14:06:02.882794|\n",
      "|b   |bike    |2015-02-24 14:01:44.679329|2015-02-24 14:08:09.5051  |\n",
      "|b   |bike    |2015-02-24 14:01:45.298684|2015-02-24 14:08:09.535312|\n",
      "|b   |bike    |2015-02-24 14:01:44.105233|2015-02-24 14:08:09.565524|\n",
      "|b   |bike    |2015-02-24 14:01:44.331887|2015-02-24 14:08:09.616062|\n",
      "|b   |bike    |2015-02-24 14:01:44.346871|2015-02-24 14:08:09.641238|\n",
      "|c   |bike    |2015-02-23 12:40:27.495645|2015-02-23 13:15:54.817554|\n",
      "|c   |bike    |2015-02-23 12:40:27.656625|2015-02-23 13:15:55.092517|\n",
      "|c   |bike    |2015-02-23 12:40:27.495645|2015-02-23 13:15:55.114611|\n",
      "|c   |bike    |2015-02-23 12:40:27.516641|2015-02-23 13:15:55.152758|\n",
      "|c   |bike    |2015-02-23 12:40:27.495645|2015-02-23 13:15:55.200152|\n",
      "|d   |bike    |2015-02-24 13:07:30.872905|2015-02-24 13:42:28.718592|\n",
      "|d   |bike    |2015-02-24 13:07:30.872905|2015-02-24 13:42:28.748866|\n",
      "|d   |bike    |2015-02-24 13:07:30.872905|2015-02-24 13:42:28.779078|\n",
      "|d   |bike    |2015-02-24 13:07:30.872905|2015-02-24 13:42:28.829341|\n",
      "|d   |bike    |2015-02-24 13:07:30.872905|2015-02-24 13:42:28.854579|\n",
      "+----+--------+--------------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM events_per_window order by user, end\").show(20, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM events_per_window order by user, start\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active.stop\n",
    "active.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 : 카운트 기반 윈도우\n",
    "\n",
    "+ 500개씩 누적될 때마다 결과를 출력\n",
    "    + device와 timestamp 필드가 있는 입력용 케이스 클래스\n",
    "    + 수집된 레코드의 현재 수, 장비 ID, 그리고 윈도우에서 읽은 이벤트의 배열을 가진 상태 케이스 클래스\n",
    "    + 출력용 케이스 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class InputRow\n",
       "defined class DeviceState\n",
       "defined class OutputRow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "case class InputRow(device: String, timestamp: java.sql.Timestamp, x: Double)\n",
    "case class DeviceState(device: String, var values: Array[Double], var count: Int)\n",
    "case class OutputRow(device: String, previousAverage: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateWithEvent: (state: DeviceState, input: InputRow)DeviceState\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def updateWithEvent(state:DeviceState, input:InputRow):DeviceState = {\n",
    "    state.count += 1\n",
    "    \n",
    "    state.values = state.values ++ Array(input.x)\n",
    "    state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateAcrossEvents: (device: String, inputs: Iterator[InputRow], oldState: org.apache.spark.sql.streaming.GroupState[DeviceState])Iterator[OutputRow]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}\n",
    "\n",
    "def updateAcrossEvents(device:String,\n",
    "                       inputs:Iterator[InputRow],\n",
    "                       oldState:GroupState[DeviceState]):Iterator[OutputRow] = {\n",
    "    \n",
    "    inputs.toSeq.sortBy(_.timestamp.getTime).toIterator.flatMap { input =>\n",
    "        val state = if (oldState.exists) oldState.get\n",
    "                    else DeviceState(device, Array(), 0)\n",
    "        \n",
    "        val newState = updateWithEvent(state, input)\n",
    "        if (newState.count >= 500) {\n",
    "            // 윈도우 중 하나가 완료되면 빈 DeviceState로 교체\n",
    "            // 과거 상태값에서 지난 500개 아이템의 평균을 구한 후 출력\n",
    "            oldState.update(DeviceState(device, Array(), 0))\n",
    "            Iterator(OutputRow(device,\n",
    "                               newState.values.sum / newState.values.length.toDouble))\n",
    "        }\n",
    "        else {\n",
    "            // 현재 DeviceState 객체로 업데이트하며 레코드를 출력하지 않음\n",
    "            oldState.update(newState)\n",
    "            Iterator()\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activeStreamQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1f5eead0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1f5eead0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.GroupStateTimeout\n",
    "\n",
    "val activeStreamQuery = withEventTime\n",
    "    .selectExpr(\"Device as device\", \"cast(Creation_time/1000000000 as timestamp) as timestamp\", \"x\")\n",
    "    .as[InputRow]\n",
    "    .groupByKey(_.device)\n",
    "    .flatMapGroupsWithState(OutputMode.Append,GroupStateTimeout.NoTimeout)(updateAcrossEvents)\n",
    "    .writeStream\n",
    "    .queryName(\"count_based_device\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------+\n",
      "|device  |previousAverage       |\n",
      "+--------+----------------------+\n",
      "|nexus4_1|9.618530235999994E-4  |\n",
      "|nexus4_1|-6.832885523999994E-4 |\n",
      "|nexus4_1|3.549194291999999E-4  |\n",
      "|nexus4_1|-6.747436750000009E-4 |\n",
      "|nexus4_1|4.38232380200002E-4   |\n",
      "|nexus4_1|9.082031224E-4        |\n",
      "|nexus4_1|4.937744130000001E-4  |\n",
      "|nexus4_1|1.6693114360000024E-4 |\n",
      "|nexus4_1|-7.836914039999995E-4 |\n",
      "|nexus4_1|6.668090874000004E-4  |\n",
      "|nexus4_1|-1.2573242699999968E-4|\n",
      "|nexus4_1|6.86035155E-4         |\n",
      "|nexus4_1|3.0578613100000035E-4 |\n",
      "|nexus4_1|0.007859497038599986  |\n",
      "|nexus4_1|0.001040649405199999  |\n",
      "|nexus4_1|-6.74743674200001E-4  |\n",
      "|nexus4_1|-6.597900420000009E-4 |\n",
      "|nexus4_1|-1.2786865039999968E-4|\n",
      "|nexus4_1|6.439209080000028E-5  |\n",
      "|nexus4_1|7.037352340000015E-5  |\n",
      "+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM count_based_device\").show(20, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1f5eead0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d9e3b9ec-c0e3-4b9e-993d-c1c43cc1a37d"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activeStreamQuery.runId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activeStreamQuery.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activeStreamQuery.stop\n",
    "activeStreamQuery.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22.7.4 flatMapGroupsWithState\n",
    "+ 필요 요소\n",
    "    + 세 가지 클래스 정의 : 입력 클래스, 상태 클래스, 출력 클래스(선택)\n",
    "    + 키, 이벤트 이터레이터 그리고 이전 상태를 기반으로 상태를 갱신하는 함수\n",
    "    + 타임아웃 파라미터 (22.7.1절)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 : 세션화\n",
    "+ 사용자 ID와 일부 시간정보를 이용해 실행 시점에서 세션을 생성함\n",
    "+ 5초안에 해당 사용자가 새로운 이벤트를 만들지 않으면 세션을 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class InputRow\n",
       "defined class UserSession\n",
       "defined class UserSessionOutput\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "case class InputRow(uid:String, timestamp:java.sql.Timestamp, x:Double, activity:String)\n",
    "case class UserSession(val uid:String, var timestamp:java.sql.Timestamp, var activities: Array[String], var values: Array[Double])\n",
    "case class UserSessionOutput(val uid:String, var activities: Array[String], var xAvg:Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateWithEvent: (state: UserSession, input: InputRow)UserSession\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def updateWithEvent(state:UserSession, input:InputRow):UserSession = {\n",
    "    // 비정상 날짜\n",
    "    if (Option(input.timestamp).isEmpty) {\n",
    "        return state\n",
    "    }\n",
    "    \n",
    "    state.timestamp = input.timestamp\n",
    "    state.values = state.values ++ Array(input.x)\n",
    "    if (!state.activities.contains(input.activity)) {\n",
    "      state.activities = state.activities ++ Array(input.activity)\n",
    "    }\n",
    "    state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateAcrossEvents: (uid: String, inputs: Iterator[InputRow], oldState: org.apache.spark.sql.streaming.GroupState[UserSession])Iterator[UserSessionOutput]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}\n",
    "\n",
    "def updateAcrossEvents(uid:String, inputs: Iterator[InputRow],\n",
    "    oldState: GroupState[UserSession]):Iterator[UserSessionOutput] = {\n",
    "\n",
    "    inputs.toSeq.sortBy(_.timestamp.getTime).toIterator.flatMap { input =>\n",
    "        val state = if (oldState.exists) oldState.get\n",
    "                    else UserSession(uid,\n",
    "                                     new java.sql.Timestamp(6284160000000L),\n",
    "                                     Array(),\n",
    "                                     Array())\n",
    "        val newState = updateWithEvent(state, input)\n",
    "        if (oldState.hasTimedOut) {\n",
    "            val state = oldState.get\n",
    "            oldState.remove()\n",
    "            Iterator(UserSessionOutput(uid,\n",
    "                                       state.activities,\n",
    "                                       newState.values.sum / newState.values.length.toDouble))\n",
    "        }\n",
    "        else if (state.values.length > 1000) {\n",
    "            val state = oldState.get\n",
    "            oldState.remove()\n",
    "            Iterator(UserSessionOutput(uid,\n",
    "                                       state.activities,\n",
    "                                       newState.values.sum / newState.values.length.toDouble))\n",
    "        }\n",
    "        else {\n",
    "            oldState.update(newState)\n",
    "            oldState.setTimeoutTimestamp(newState.timestamp.getTime(), \"5 seconds\") // 5초 늦게 도착한 이벤트까지만 처리\n",
    "            Iterator()\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activeQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3a3c6256\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3a3c6256"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.GroupStateTimeout\n",
    "\n",
    "val activeQuery = withEventTime.where(\"x is not null\")\n",
    "    .selectExpr(\"user as uid\",\n",
    "                \"cast(Creation_Time/1000000000 as timestamp) as timestamp\",\n",
    "                \"x\", \"gt as activity\")\n",
    "    .as[InputRow]\n",
    "    .withWatermark(\"timestamp\", \"5 seconds\")\n",
    "    .groupByKey(_.uid)\n",
    "    .flatMapGroupsWithState(OutputMode.Append,\n",
    "                            GroupStateTimeout.EventTimeTimeout)(updateAcrossEvents)\n",
    "    .writeStream\n",
    "    .queryName(\"count_based_device\")\n",
    "    .format(\"memory\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------------------+\n",
      "|uid|activities        |xAvg                  |\n",
      "+---+------------------+----------------------+\n",
      "|a  |[stand]           |0.001636105028871131  |\n",
      "|a  |[stand]           |-0.0023759961309690303|\n",
      "|a  |[stand]           |3.791222360639342E-4  |\n",
      "|a  |[stand]           |-0.003535877572927077 |\n",
      "|a  |[stand]           |0.0017129324781218753 |\n",
      "|a  |[stand]           |-3.7026542657342884E-5|\n",
      "|a  |[stand, null, sit]|-0.0028615030874125846|\n",
      "|a  |[sit]             |1.7531602997002986E-4 |\n",
      "|a  |[sit]             |-4.881745294705291E-4 |\n",
      "|a  |[sit]             |-2.7621303606393583E-4|\n",
      "|a  |[sit]             |1.4277103326673296E-4 |\n",
      "|a  |[sit]             |8.940339170829161E-5  |\n",
      "|a  |[sit]             |1.7837997412587433E-4 |\n",
      "|a  |[sit]             |4.268186813187034E-6  |\n",
      "|a  |[sit, null]       |-1.0136957722277705E-4|\n",
      "|a  |[null]            |-0.03021667061188812  |\n",
      "|a  |[null, walk]      |-0.06103302192717283  |\n",
      "|a  |[walk]            |-0.02639023600509492  |\n",
      "|a  |[walk]            |0.019427746320279705  |\n",
      "|a  |[walk]            |3.980090740259678E-4  |\n",
      "+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM count_based_device\").show(20, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activeQuery.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"message\" : \"Waiting for data to arrive\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : false\n",
       "}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "activeQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "  \"id\" : \"cd5a2bdf-ac33-43dd-966f-83f376e4cd69\",\n",
       "  \"runId\" : \"7b0462bd-0438-4f86-afdc-bf481046e1b5\",\n",
       "  \"name\" : \"count_based_device\",\n",
       "  \"timestamp\" : \"2020-06-27T02:47:13.864Z\",\n",
       "  \"batchId\" : 0,\n",
       "  \"numInputRows\" : 780119,\n",
       "  \"processedRowsPerSecond\" : 157313.77293809236,\n",
       "  \"durationMs\" : {\n",
       "    \"addBatch\" : 4723,\n",
       "    \"getBatch\" : 10,\n",
       "    \"getOffset\" : 48,\n",
       "    \"queryPlanning\" : 128,\n",
       "    \"triggerExecution\" : 4959,\n",
       "    \"walCommit\" : 30\n",
       "  },\n",
       "  \"eventTime\" : {\n",
       "    \"avg\" : \"2015-02-24T03:00:44.892Z\",\n",
       "    \"max\" : \"2015-02-24T15:21:45.941Z\",\n",
       "    \"min\" : \"2015-02-22T00:41:40.120Z\",\n",
       "    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n",
       "  },\n",
       "  \"stateOperators\" : [ {\n",
       "    \"numRowsTotal\" : 9,\n",
       "    \"numRowsUpdated\" : 9,\n",
       "    \"memoryUsedB...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "activeQuery.recentProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activeQuery.stop\n",
    "activeQuery.isActive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
