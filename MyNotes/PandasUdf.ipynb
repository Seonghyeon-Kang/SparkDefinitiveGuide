{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Pandas UDFs and Python Type Hints in the Upcoming Release of Apache Spark 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow version is 0.17.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dd15a0dde9ac:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Pandas UDF in Spark 3.0</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f385e012040>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "print(f\"pyarrow version is {pa.__version__}\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Python Pandas UDF in Spark 3.0\")\n",
    "    .config('spark.sql.shuffle.partitions', 5)\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Pandas UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator of Series to Iterator of Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  2|\n",
      "|                  3|\n",
      "|                  4|\n",
      "|                  5|\n",
      "|                  6|\n",
      "|                  7|\n",
      "|                  8|\n",
      "|                  9|\n",
      "|                 10|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    return map(lambda s: s + 1, iterator)\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pandas_udf(\"long\")\n",
    "# def calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "#     # Do some expensive initialization with a state\n",
    "#     state = very_expensive_initialization()\n",
    "#     for x in iterator:\n",
    "#         # Use that state for the whole iterator.\n",
    "#         yield calculate_with_state(x, state)\n",
    "\n",
    "# df.select(calculate(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator of Multiple Series to Iterator of Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|multiply_two(id, id)|\n",
      "+--------------------+\n",
      "|                   0|\n",
      "|                   1|\n",
      "|                   4|\n",
      "|                   9|\n",
      "|                  16|\n",
      "|                  25|\n",
      "|                  36|\n",
      "|                  49|\n",
      "|                  64|\n",
      "|                  81|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, Tuple\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf       \n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def multiply_two(\n",
    "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    return (a * b for a, b in iterator)\n",
    "\n",
    "spark.range(10).select(multiply_two(\"id\", \"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series to Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|pandas_mean(v)|\n",
      "+--------------+\n",
      "|          21.0|\n",
      "+--------------+\n",
      "\n",
      "+---+--------------+\n",
      "| id|pandas_mean(v)|\n",
      "+---+--------------+\n",
      "|  2|          18.0|\n",
      "|  1|           3.0|\n",
      "+---+--------------+\n",
      "\n",
      "+---------------------------------------------------------+\n",
      "|pandas_mean(v) OVER (PARTITION BY id unspecifiedframe$())|\n",
      "+---------------------------------------------------------+\n",
      "|                                                     18.0|\n",
      "|                                                     18.0|\n",
      "|                                                     18.0|\n",
      "|                                                      3.0|\n",
      "|                                                      3.0|\n",
      "+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def pandas_mean(v: pd.Series) -> float:\n",
    "    return v.sum()\n",
    "\n",
    "df.select(pandas_mean(df['v'])).show()\n",
    "df.groupby(\"id\").agg(pandas_mean(df['v'])).show()\n",
    "df.select(pandas_mean(df['v']).over(Window.partitionBy('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Pandas Function APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "def subtract_mean(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n",
    "\n",
    "df.groupby(\"id\").applyInPandas(subtract_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1| 21|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
    "\n",
    "def pandas_filter(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]\n",
    "\n",
    "df.mapInPandas(pandas_filter, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-grouped Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+\n",
      "|time| id| v1| v2|\n",
      "+----+---+---+---+\n",
      "|1201|  2|2.0|  y|\n",
      "|1202|  2|4.0|  y|\n",
      "|1201|  1|1.0|  x|\n",
      "|1202|  1|3.0|  x|\n",
      "+----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = spark.createDataFrame(\n",
    "    [(1201, 1, 1.0), (1201, 2, 2.0), (1202, 1, 3.0), (1202, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "df2 = spark.createDataFrame(\n",
    "    [(1201, 1, \"x\"), (1201, 2, \"y\")], (\"time\", \"id\", \"v2\"))\n",
    "\n",
    "def asof_join(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge_asof(left, right, on=\"time\", by=\"id\")\n",
    "\n",
    "df1.groupby(\"id\").cogroup(\n",
    "    df2.groupby(\"id\")\n",
    ").applyInPandas(asof_join, \"time int, id int, v1 double, v2 string\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
